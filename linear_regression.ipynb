{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bycicle Counter Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "- prohibited to select on the month of july\n",
    "\n",
    "## TO DO\n",
    "\n",
    "- check for assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download extended bycicle counter datasets (!wget if not on UNIX)\n",
    "!curl 'https://data.stad.gent/api/explore/v2.1/catalog/datasets/fietstelpaal-coupure-links-2023-gent/exports/csv?lang=en&timezone=Europe%2FBrussels&use_labels=true&delimiter=%3B' -o 'data/fiets_2023.csv'\n",
    "!curl 'https://data.stad.gent/api/explore/v2.1/catalog/datasets/fietstelpaal-coupure-links-2022-gent/exports/csv?lang=en&timezone=Europe%2FBrussels&use_labels=true&delimiter=%3B' -o 'data/fiets_2022.csv'\n",
    "!curl 'https://data.stad.gent/api/explore/v2.1/catalog/datasets/fietstelpaal-coupure-links-2021-gent/exports/csv?lang=en&timezone=Europe%2FBrussels&use_labels=true&delimiter=%3B' -o 'data/fiets_2021.csv'\n",
    "\n",
    "# Download weather data\n",
    "!curl 'https://archive-api.open-meteo.com/v1/archive?latitude=51.100006&longitude=3.699997&start_date=2021-03-01&end_date=2023-07-31&hourly=temperature_2m,precipitation,rain,snowfall,snow_depth,cloudcover,windspeed_10m,windgusts_10m&format=csv' -o 'data/weather_data_full.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "fiets_2023 = pd.read_csv('./data/fiets_2023.csv',delimiter=';')\n",
    "fiets_2022 = pd.read_csv('./data/fiets_2022.csv',delimiter=';')\n",
    "fiets_2021 = pd.read_csv('./data/fiets_2021.csv',delimiter=';')\n",
    "\n",
    "weather_data = pd.read_csv('./data/weather_data_full.csv',skiprows=3)\n",
    "test_data = pd.read_csv('./data/test_data.csv')\n",
    "\n",
    "# concatenate the training data\n",
    "train_data = pd.concat([fiets_2021, fiets_2022, fiets_2023], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time columns to pandas datetime format\n",
    "## test data\n",
    "test_data['datetime'] = pd.to_datetime(test_data['Date_hour'])\n",
    "\n",
    "## train data\n",
    "train_data['datetime'] = pd.to_datetime(train_data['Datum']+ ' ' + train_data['Uur5Minuten'])\n",
    "\n",
    "## weather data\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant time columns\n",
    "train_data = train_data[['datetime','Totaal']]\n",
    "weather_data = weather_data.drop('time',axis =1)\n",
    "test_data = test_data.drop('Date_hour',axis=1)\n",
    "\n",
    "# Remove datapoints later than 30th of june 2023\n",
    "train_data = train_data[train_data['datetime'] <= '2023-06-30 23:00']\n",
    "\n",
    "# Group by hour\n",
    "train_data = train_data.groupby(train_data['datetime'].dt.strftime('%Y-%m-%d %H:00'))['Totaal'].sum()\n",
    "\n",
    "# reset index\n",
    "train_data = train_data.reset_index()\n",
    "\n",
    "# reset \"datetime\" column to pandas datetime format as this changes to \"object\" type during execution of the \"groupby\" function.\n",
    "train_data['datetime'] = pd.to_datetime(train_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature_2m (°C)</th>\n",
       "      <th>precipitation (mm)</th>\n",
       "      <th>rain (mm)</th>\n",
       "      <th>snowfall (cm)</th>\n",
       "      <th>snow_depth (m)</th>\n",
       "      <th>cloudcover (%)</th>\n",
       "      <th>windspeed_10m (km/h)</th>\n",
       "      <th>windgusts_10m (km/h)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>18.8</td>\n",
       "      <td>37.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>20.4</td>\n",
       "      <td>39.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>21.9</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>21.7</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>20.9</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>2023-07-31 19:00:00</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>17.5</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>2023-07-31 20:00:00</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>2023-07-31 21:00:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82</td>\n",
       "      <td>17.1</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>2023-07-31 22:00:00</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "      <td>16.3</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>2023-07-31 23:00:00</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>16.1</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime  temperature_2m (°C)  precipitation (mm)  rain (mm)  \\\n",
       "Id                                                                            \n",
       "0   2023-07-01 00:00:00                 18.0                 0.0        0.0   \n",
       "1   2023-07-01 01:00:00                 17.7                 0.0        0.0   \n",
       "2   2023-07-01 02:00:00                 17.4                 0.0        0.0   \n",
       "3   2023-07-01 03:00:00                 17.2                 0.0        0.0   \n",
       "4   2023-07-01 04:00:00                 17.2                 0.0        0.0   \n",
       "..                  ...                  ...                 ...        ...   \n",
       "739 2023-07-31 19:00:00                 17.6                 0.4        0.4   \n",
       "740 2023-07-31 20:00:00                 17.3                 0.1        0.1   \n",
       "741 2023-07-31 21:00:00                 17.0                 0.1        0.1   \n",
       "742 2023-07-31 22:00:00                 16.5                 0.1        0.1   \n",
       "743 2023-07-31 23:00:00                 16.1                 0.1        0.1   \n",
       "\n",
       "     snowfall (cm)  snow_depth (m)  cloudcover (%)  windspeed_10m (km/h)  \\\n",
       "Id                                                                         \n",
       "0              0.0             0.0             100                  18.8   \n",
       "1              0.0             0.0             100                  20.4   \n",
       "2              0.0             0.0             100                  21.9   \n",
       "3              0.0             0.0             100                  21.7   \n",
       "4              0.0             0.0             100                  20.9   \n",
       "..             ...             ...             ...                   ...   \n",
       "739            0.0             0.0             100                  17.5   \n",
       "740            0.0             0.0              98                  17.0   \n",
       "741            0.0             0.0              82                  17.1   \n",
       "742            0.0             0.0              65                  16.3   \n",
       "743            0.0             0.0              44                  16.1   \n",
       "\n",
       "     windgusts_10m (km/h)  \n",
       "Id                         \n",
       "0                    37.1  \n",
       "1                    39.6  \n",
       "2                    41.0  \n",
       "3                    41.8  \n",
       "4                    41.8  \n",
       "..                    ...  \n",
       "739                  30.2  \n",
       "740                  31.7  \n",
       "741                  32.8  \n",
       "742                  30.6  \n",
       "743                  28.8  \n",
       "\n",
       "[744 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge weather data with training data and test data, based on timepoints\n",
    "train_data = pd.merge(train_data, weather_data, on='datetime', how='inner')\n",
    "test_data = pd.merge(test_data, weather_data, on='datetime', how='inner')\n",
    "\n",
    "# index\n",
    "test_data.set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic info\n",
    "- Correlation\n",
    "- PCA is unnessecary as we aren't dealing with a large number of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic statistics and data structure\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features (excluding 'datetime')\n",
    "numerical_features = train_data.select_dtypes(include='number')\n",
    "\n",
    "# Histograms\n",
    "numerical_features.hist(figsize=(10, 10),bins=30)\n",
    "plt.suptitle(\"Histograms\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplots\n",
    "numerical_features.plot(kind='box', subplots=True, figsize=(10, 10),layout=(3, 3))\n",
    "plt.suptitle(\"Box Plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = train_data.corr()['Totaal'].drop('datetime')\n",
    "print(correlation.abs().sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = train_data.corr()\n",
    "sns.heatmap(train_data.corr().drop(columns=['Totaal','datetime']), annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Low correlation with 'Totaal' in multiple variables - those will be removed.\n",
    "- Strong multicollinearity between precipitation-rain and windspeed-windgusts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "sns.pairplot(train_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes prompted by exploration\n",
    "train_data = train_data.drop(columns=['snow_depth (m)','windspeed_10m (km/h)','cloudcover (%)','precipitation (mm)','rain (mm)','snowfall (cm)'])\n",
    "test_data = test_data.drop(columns=['snow_depth (m)','windspeed_10m (km/h)','cloudcover (%)','precipitation (mm)','rain (mm)','snowfall (cm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gentse feesten 2022\n",
    "gentse_feesten_start_2022 = pd.to_datetime('2022-07-15')\n",
    "gentse_feesten_end_2022 = pd.to_datetime('2022-07-24')\n",
    "\n",
    "train_data['is_gentse_feesten_active'] = (\n",
    "    (train_data['datetime'] >= gentse_feesten_start_2022) &\n",
    "    (train_data['datetime'] <= gentse_feesten_end_2022)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# Gentse feesten 2023\n",
    "gentse_feesten_start_2023 = pd.to_datetime('2023-07-14')\n",
    "gentse_feesten_end_2023 = pd.to_datetime('2023-07-23')\n",
    "\n",
    "test_data['is_gentse_feesten_active'] = (\n",
    "    (test_data['datetime'] >= gentse_feesten_start_2023) &\n",
    "    (test_data['datetime'] <= gentse_feesten_end_2023)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. train_data\n",
    "\n",
    "# Hour of the day\n",
    "train_data['hour'] = train_data['datetime'].dt.hour\n",
    "train_data = pd.get_dummies(train_data, columns=['hour'], prefix='hour', prefix_sep='_')\n",
    "\n",
    "# Day of the week\n",
    "days_dummies = pd.get_dummies(train_data['datetime'].dt.dayofweek, prefix='day', prefix_sep='_')\n",
    "train_data = pd.concat([train_data, days_dummies], axis=1)\n",
    "\n",
    "# Months - dummy variables\n",
    "months_dummies = pd.get_dummies(train_data['datetime'].dt.month, prefix='month')\n",
    "train_data = pd.concat([train_data, months_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. test_data\n",
    "\n",
    "# Extract hours into dummy variables\n",
    "test_data['hour'] = test_data['datetime'].dt.hour\n",
    "test_data = pd.get_dummies(test_data, columns=['hour'], prefix='hour', prefix_sep='_')\n",
    "\n",
    "# Day of the week\n",
    "days_dummies = pd.get_dummies(test_data['datetime'].dt.dayofweek, prefix='day', prefix_sep='_')\n",
    "test_data = pd.concat([test_data, days_dummies], axis=1)\n",
    "\n",
    "# months:\n",
    "months_dummies = pd.get_dummies(test_data['datetime'].dt.month, prefix='month')\n",
    "test_data = pd.concat([test_data, months_dummies], axis=1)\n",
    "\n",
    "\n",
    "missing_months = [column for column in set(train_data.columns) if column.startswith(\"month\") and column not in set(test_data.columns)]\n",
    "for column in missing_months:\n",
    "    test_data[column] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding weekend and summer vacation period is redundant as the variance is already explained by the days and months. It would result multicollinearity with months 7 and 8 and days 6 and 7.\n",
    "- Only adding a variable for weekday vs weekend did not generate a better performance than using dummy variables for all days.\n",
    "- Attempted capturing cyclic behavior of the hour of the day and day of the week by encoding as sin/cos values. This did not improve the model's performance.\n",
    "- Added interaction terms to the model, this did not improve the model's performance either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop datetime columns\n",
    "train_data = train_data.drop('datetime', axis=1)\n",
    "test_data = test_data.drop('datetime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data exploration - revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totaal                      1.000000\n",
      "hour_cos                    0.474936\n",
      "hour_sin                    0.292305\n",
      "day_sin                     0.252241\n",
      "temperature_2m (°C)         0.222536\n",
      "windgusts_10m (km/h)        0.075790\n",
      "day_cos                     0.069820\n",
      "inter                       0.026807\n",
      "is_gentse_feesten_active    0.022801\n",
      "Name: Totaal, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation = train_data.corr()['Totaal']\n",
    "print(correlation.abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These new time-related variables show decent correlation with the number of cyclists and will be retained in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in numpy arrays\n",
    "y = train_data.Totaal.values\n",
    "X = train_data.drop('Totaal', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test on own data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7) # Use 70% of data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "LinReg = LinearRegression() # call an instance of the class LinearRegression\n",
    "\n",
    "LinReg.fit(X_train, y_train) # fit the model on the training data\n",
    "y_hat_train = LinReg.predict(X_train) # predict training data\n",
    "MSE_train = mean_squared_error(y_train, y_hat_train) # Compute training set MSE\n",
    "\n",
    "y_hat_test = LinReg.predict(X_test) # predict test data\n",
    "MSE_test = mean_squared_error(y_test, y_hat_test) # Test set MSE\n",
    "\n",
    "R_train = LinReg.score(X_train, y_train) # Training set R²\n",
    "R_test = LinReg.score(X_test, y_test) # Test set R²\n",
    "\n",
    "print('Training set MSE: {}'.format(MSE_train))\n",
    "print('Test set MSE: {}'.format(MSE_test))\n",
    "print('Train set R²: {}'.format(R_train))\n",
    "print('Test set R²: {}'.format(R_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## gridsearch\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    # Add other hyperparameters\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    XGBRegressor(objective='reg:squarederror'),\n",
    "    param_grid=param_grid,\n",
    "    cv=5  # Number of cross-validation folds\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## random search\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0, 1),\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(1, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    # Add other hyperparameters\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    XGBRegressor(objective='reg:squarederror'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of random combinations to try\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 19803.053024151526\n",
      "R-squared: 0.7541775363584107\n"
     ]
    }
   ],
   "source": [
    "## for own data\n",
    "\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for kaggle data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_data.drop(\"Id\", axis=1))\n",
    "\n",
    "## y_pred[y_pred < 0] = 0\n",
    "\n",
    "# format predictions with Ids into dataframe and save to csv.\n",
    "submission_file = pd.DataFrame([test_data[\"Id\"], y_pred]).T\n",
    "submission_file.columns = [\"Id\", \"Predicted\"]\n",
    "\n",
    "submission_file.to_csv(\"submission.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts:\n",
    "- linear regression\n",
    "- polynomial terms\n",
    "- random forest regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
